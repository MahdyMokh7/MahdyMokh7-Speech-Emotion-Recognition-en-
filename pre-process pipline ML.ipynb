{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process Pipeline\n",
    "\n",
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy.signal\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "MAIN_FOLDER_PATH = \"..\"\n",
    "DATASET_PATH = os.path.join(MAIN_FOLDER_PATH, \"DataSet\")\n",
    "OUTPUT_FOLDER = os.path.join(MAIN_FOLDER_PATH, \"PreProcessedDataSet_for_ML\")\n",
    "TARGET_SR = 16000   # sapmle rate\n",
    "SILENCE_TOP_DB = 40\n",
    "HIGHPASS_CUTOFF = 60  # cutoff to remove low-frequency noise\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Functions For Pre-processing Explained\n",
    "\n",
    "The reason for using these specific functinsfor our pre-processing pipeline for *traditional ML speech emotion recognition* on the **CREMA-D dataset**:\n",
    "\n",
    "---\n",
    "\n",
    "#### **`load_and_resample_audio(file_path, sr=TARGET_SR)`**  \n",
    "**Reason for using:**  \n",
    "- Ensures all audio files have the same sampling rate (e.g., 16kHz).  \n",
    "- Different recordings might have varying sample rates, which makes feature extraction (like MFCC, Chroma, etc.) inconsistent.  \n",
    "- Having a consistent `sr` allows uniform time and frequency resolution for all extracted features.  \n",
    "\n",
    "*In short: Without uniform sampling rate, features would not be comparable.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **`convert_to_mono(audio)`**  \n",
    "**Reason for using:**  \n",
    "- CREMA-D clips can be stereo.  \n",
    "- Traditional audio feature extraction (MFCC, Chroma, Spectral Contrast) works on mono signals — stereo channels can cause duplication and confusion.  \n",
    "- Mono conversion simplifies the signal and avoids unnecessary complexity.  \n",
    "\n",
    "*Mono ensures one clean signal for consistent, meaningful feature extraction.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **`trim_silence(audio, top_db=SILENCE_TOP_DB)`**  \n",
    "**Reason for using:**  \n",
    "- Light silence trimming (e.g., `top_db=40`) removes long, unnecessary silences at the beginning and end.  \n",
    "- Long silences would inflate feature lengths or distort statistics (for features like zero-crossing rate or spectral entropy).  \n",
    "- But trimming is gentle, so we don’t remove emotional pauses inside the speech.  \n",
    "\n",
    "*Helps keep the audio focused on speech content, improving feature quality.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **`normalize_audio(audio)`**  \n",
    "**Reason for using:**  \n",
    "- Normalizes audio so that its peak amplitude is 1.  \n",
    "- Feature extractors are amplitude-sensitive (especially energy-based features).  \n",
    "- Without normalization, two identical speech samples at different recording volumes could look like different emotional intensities.  \n",
    "- Peak normalization maintains relative loudness dynamics but makes the scale consistent.  \n",
    "\n",
    "*Eliminates unwanted loudness variations while preserving natural expressive dynamics.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **`apply_highpass_filter(audio, sr, cutoff=HIGHPASS_CUTOFF)`**  \n",
    "**Reason for using:**  \n",
    "- Removes low-frequency hums and rumble below 80Hz (e.g., air conditioning noise, mic stand vibrations).  \n",
    "- These low frequencies are *not useful* for emotion; emotion features live in mid- and high-frequency ranges (pitch, formants, harmonics).  \n",
    "- Prevents “pollution” of features like MFCC or spectral contrast by meaningless low-frequency noise.  \n",
    "\n",
    "*Keeps the frequency content clean and relevant for speech emotion features.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== PREPROCESSING FUNCTIONS ===================\n",
    "\n",
    "def load_and_resample_audio(file_path, sr=TARGET_SR):\n",
    "    \"\"\"Load audio file and resample.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "def convert_to_mono(audio):\n",
    "    \"\"\"Convert stereo audio to mono.\"\"\"\n",
    "    return librosa.to_mono(audio)\n",
    "\n",
    "def trim_silence(audio, top_db=SILENCE_TOP_DB):\n",
    "    \"\"\"Trim leading and trailing silence (if any).\"\"\"\n",
    "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db)\n",
    "    return trimmed_audio\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    \"\"\"Normalize audio amplitude to peak value of 1.\"\"\"\n",
    "    return audio / np.max(np.abs(audio))\n",
    "\n",
    "def apply_highpass_filter(audio, sr, cutoff=HIGHPASS_CUTOFF):\n",
    "    \"\"\"Apply high-pass filter to remove low-frequency noise.\"\"\"\n",
    "    sos = scipy.signal.butter(10, cutoff, btype='highpass', fs=sr, output='sos')\n",
    "    return scipy.signal.sosfilt(sos, audio)\n",
    "\n",
    "def is_not_empty(audio):\n",
    "    return np.max(np.abs(audio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Functions We Could Have Used But Avoided and Why\n",
    "\n",
    "The below functions that you will see on the next python cell are the function that we could've used but decided not to use for the following reasons:\n",
    "\n",
    "#### **Noise Reduction**  \n",
    "\n",
    "**Why we avoided it:**  \n",
    "- CREMA-D is a *studio-recorded* dataset with relatively clean audio;  \n",
    "- Noise reduction algorithms can sometimes distort the natural tone of speech, which is critical for emotion recognition;  \n",
    "- Risk of over-smoothing, harming features like pitch variance or spectral dynamics that are crucial for emotion detection.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Extreme Silence Removal (aggressive trimming)**  \n",
    "\n",
    "**Why we avoided it:**  \n",
    "- Too aggressive silence removal could cut natural pauses that carry emotional cues (hesitation, sighs, or dramatic pauses).  \n",
    "- Emotion detection benefits from subtle prosody and timing variations.  \n",
    "- We stuck to `top_db=40` light trimming instead.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Volume Standardization (RMS normalization)**  \n",
    "\n",
    "**Why avoided:**  \n",
    "- RMS normalization can distort amplitude-based features like loudness, which are *strong indicators* of emotion (e.g., anger vs. calm).  \n",
    "- Peak normalization was chosen to preserve relative dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Band-Pass Filtering (instead of high-pass)**  \n",
    "\n",
    "**Why avoided:**  \n",
    "- CREMA-D recordings already have controlled frequency ranges.  \n",
    "- Over-filtering could remove important frequency components (high frequencies carry stress, low frequencies carry warmth).  \n",
    "- High-pass filter (80Hz) was enough to clean up low rumble without cutting expressive content.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Time-Stretch or Pitch-Shifting (Data Augmentation)**  \n",
    "\n",
    "These are great for training large neural models but **not suitable** at preprocessing for ML feature extraction.  \n",
    "- Feature extraction should happen on clean, original audio.  \n",
    "- Augmentation could introduce bias or misrepresent real emotional cues.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== PREPROCESSING FUNCTIONS WE DIDN'T USE ===================\n",
    "\n",
    "def reduce_noise(audio, sr):\n",
    "    \"\"\"Perform noise reduction using spectral gating.\"\"\"\n",
    "    return nr.reduce_noise(y=audio, sr=sr)\n",
    "\n",
    "def aggressive_trim(audio, top_db=20):\n",
    "    \"\"\"Remove even quieter silences more aggressively.\"\"\"\n",
    "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db)\n",
    "    return trimmed_audio\n",
    "\n",
    "def rms_normalize(audio):\n",
    "    \"\"\"Normalize based on RMS energy.\"\"\"\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return audio / rms\n",
    "\n",
    "def bandpass_filter(audio, sr, lowcut=80, highcut=8000):\n",
    "    \"\"\"Apply band-pass filter.\"\"\"\n",
    "    sos = scipy.signal.butter(10, [lowcut, highcut], btype='band', fs=sr, output='sos')\n",
    "    return scipy.signal.sosfilt(sos, audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the pipeline process. It gets the audio file does all the processing functions we chose that where suitable and important to do and after processing the audio file the file will be saved as a new file in another foler where all the preProcessed .wav files are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== PIPELINE PROCESS FUNCTION ===================\n",
    "\n",
    "def process_audio_file(file_path):\n",
    "    \"\"\"Apply full preprocessing pipeline to a single audio file (for traditional ML).\"\"\"\n",
    "    try:\n",
    "        #print(f\"Processing: {file_path}\")\n",
    "        audio, sr = load_and_resample_audio(file_path)\n",
    "        if(is_not_empty(audio)):\n",
    "            audio = convert_to_mono(audio)\n",
    "            audio = trim_silence(audio)\n",
    "            audio = normalize_audio(audio)\n",
    "            audio = apply_highpass_filter(audio, sr)\n",
    "\n",
    "            # Note: No padding or truncation — audio length can vary for ML models\n",
    "            output_file = os.path.join(OUTPUT_FOLDER, os.path.basename(file_path))\n",
    "            sf.write(output_file, audio, sr)\n",
    "            #print(f\"Saved: {output_file}\")\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f\"Error processing {file_path}: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== MAIN PIPELINE ===================\n",
    "\n",
    "def preprocess_dataset():\n",
    "    \"\"\"Process all audio files in the dataset (traditional ML pipeline).\"\"\"\n",
    "    for filename in os.listdir(DATASET_PATH):\n",
    "        if filename.lower().endswith(\".wav\"):\n",
    "            file_path = os.path.join(DATASET_PATH, filename)\n",
    "            process_audio_file(file_path)\n",
    "    print(\"\\nAll audio files have been preprocessed and saved in: \", OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deletedddddddddddddddddddddddddddddddddddddddddddddddddddd\n",
      "\n",
      "All audio files have been preprocessed and saved in:  ..\\PreProcessedDataSet_for_ML\n"
     ]
    }
   ],
   "source": [
    "# =================== EXECUTION ===================\n",
    "preprocess_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
