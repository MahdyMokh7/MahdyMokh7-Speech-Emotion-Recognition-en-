{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of Feature Extraction Parameters:\n",
    "\n",
    "\n",
    "#### **`sr = 16000` (Sampling Rate)**  \n",
    "We use a 16 kHz sampling rate because:  \n",
    "- It is standard for speech processing tasks (also used in datasets like CREMA-D and others).  \n",
    "- Human speech rarely contains important frequency content above 8 kHz, and by Nyquist's theorem, a 16 kHz sampling rate fully captures this without unnecessary data overhead.  \n",
    "- It reduces computational load compared to 44.1 kHz or 48 kHz sampling without losing essential speech detail.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **`n_fft = 512` (FFT window size)**  \n",
    "- With `sr = 16000`, an `n_fft = 512` corresponds to a **32 ms window** (512 / 16000 = 0.032s).  \n",
    "- A 32 ms window is an excellent choice because:  \n",
    "  - It is long enough to capture steady-state speech characteristics like formants and stable spectral properties.  \n",
    "  - It still allows reasonable time resolution to capture dynamic transitions in emotion (like sudden energy changes or articulation shifts).  \n",
    "- Larger FFT windows (e.g., 1024) can smooth out short-term changes, making dynamic emotions harder to detect.  \n",
    "- Smaller FFT windows (< 512) may cause frequency resolution loss.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **`hop_length = 205` (frame step)**  \n",
    "- With `sr = 16000` and `hop_length = 205`, each step is approximately **12.8 ms**.  \n",
    "- This results in roughly **60% overlap** between consecutive windows (since 205 is about 40% of 512).  \n",
    "- Why 60% overlap?  \n",
    "  - Higher overlap ensures that no sudden changes (especially in emotional speech transitions) are missed between frames.  \n",
    "  - It smooths the temporal progression of features, allowing better tracking of continuous changes in speech energy, pitch, and articulation.  \n",
    "- Lower overlaps (e.g., 25%) can make transitions more \"jumpy\" and lose smoothness, while extremely high overlaps increase computation without significant additional benefit.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **`n_mfcc = 13` (Number of MFCC coefficients)**  \n",
    "- **13 MFCC coefficients** is the classical standard for speech processing tasks.  \n",
    "- These coefficients capture the shape of the vocal tract well and have been validated in both emotion and speech recognition tasks.  \n",
    "- Fewer coefficients (< 13) risk losing resolution of vocal features.  \n",
    "- More than 13 coefficients can start capturing noise and speaker-specific idiosyncrasies rather than emotion-relevant phonetic features.  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mfcc_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m csv_aggregated_features_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcsv_features\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maudio_aggregated_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m csv_final_features_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/csv_features/final_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m all_feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mmfcc_cols\u001b[49m \u001b[38;5;241m+\u001b[39m spec_cols \u001b[38;5;241m+\u001b[39m contrast_cols \u001b[38;5;241m+\u001b[39m chroma_cols\n\u001b[0;32m     10\u001b[0m N_FFT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;66;03m# 32 ms window \u001b[39;00m\n\u001b[0;32m     11\u001b[0m HOP_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m205\u001b[39m \u001b[38;5;66;03m# 12.8 ms step → 60% overlap\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mfcc_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "#preprocessed_dataset_path = r\"F:\\PreProcessedDataSet_for_ML\"\n",
    "preprocessed_dataset_path = r\"C:\\Users\\Mahmodiyan-PC\\Desktop\\agha alborz\\term6\\datascience\\final project\\PreProcessedDataSet_for_ML1\"\n",
    "csv_features_path = r\"F:\\csv_features\\audio_features.csv\"\n",
    "csv_aggregated_features_path = r\"F:\\csv_features\\audio_aggregated_features.csv\"\n",
    "csv_final_features_path = r\"F:/csv_features/final_features.csv\"\n",
    "\n",
    "N_FFT = 512 # 32 ms window \n",
    "HOP_LENGTH = 205 # 12.8 ms step → 60% overlap\n",
    "SR=16000 \n",
    "N_MFCC=13 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Feature Selection and Parameter Choices for Speech Emotion Recognition (CREMA-D Dataset)\n",
    "\n",
    "#### MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "\n",
    "**What is it?**  \n",
    "MFCCs represent the short-term power spectrum of speech signals based on human auditory perception (the mel scale). They capture the shape and configuration of the vocal tract, which changes with articulation.\n",
    "\n",
    "**Why do we use MFCC?**  \n",
    "The articulation patterns of speech are strongly influenced by emotion. For example, anger and excitement cause tense, sharp articulation; sadness leads to soft, dull speech. MFCCs help model these articulatory characteristics effectively.\n",
    "\n",
    "**Why use 13 coefficients?**  \n",
    "Thirteen coefficients strike a well-established balance:  \n",
    "- Fewer than 13 coefficients can underrepresent the spectrum’s important details.  \n",
    "- More than 13 may start to capture irrelevant noise or speaker identity traits rather than emotion-related changes.\n",
    "\n",
    "**Why keep both mean and standard deviation?**  \n",
    "The mean MFCC vector captures the average shape of the vocal tract across the utterance. The standard deviation highlights how much the articulation changes during the utterance. Emotional speech tends to be more dynamic (e.g., sudden shifts in anger or excitement), and the std helps measure this variability.\n",
    "\n",
    "---\n",
    "\n",
    "#### MFCC Delta and Delta-Delta\n",
    "\n",
    "**What are they?**  \n",
    "These are the first and second-order time derivatives of the MFCC coefficients, capturing the velocity and acceleration of feature changes across frames.\n",
    "\n",
    "**Why do we use delta and delta-delta?**  \n",
    "While static MFCCs represent the spectral shape, deltas and delta-deltas capture dynamic changes in articulation, which are highly emotion-dependent. For instance, sudden changes in pitch or formants are characteristic of certain emotions.\n",
    "\n",
    "**Why keep both mean and standard deviation?**  \n",
    "The mean captures the overall direction and smoothness of changes. The standard deviation measures how stable or abrupt these dynamic transitions are. Emotions like anger or surprise often show greater fluctuations, reflected well in the std.\n",
    "\n",
    "---\n",
    "\n",
    "#### Zero Crossing Rate (ZCR)\n",
    "\n",
    "**What is it?**  \n",
    "ZCR is the rate at which the signal waveform crosses the zero amplitude axis. It indirectly measures the noisiness and frequency content of the speech.\n",
    "\n",
    "**Why do we use ZCR?**  \n",
    "Energetic or tense emotions (like anger or excitement) often involve more fricatives and sharper sounds, leading to higher zero crossing rates. Calm or sad speech tends to have lower ZCR.\n",
    "\n",
    "**Why the chosen parameters?**  \n",
    "We use the same frame length (`n_fft=1024`) and `hop_length=512` as for MFCCs to maintain alignment across all features and ensure consistent temporal resolution.\n",
    "\n",
    "**Why keep both mean and standard deviation?**  \n",
    "The mean ZCR gives a stable measurement of the overall noisiness or articulation sharpness. The standard deviation helps capture frame-to-frame variability, which can differentiate between calm (steady) and agitated (fluctuating) speech.\n",
    "\n",
    "---\n",
    "\n",
    "#### RMS Energy\n",
    "\n",
    "**What is it?**  \n",
    "RMS (Root Mean Square) energy measures the loudness of the audio signal.\n",
    "\n",
    "**Why do we use RMS energy?**  \n",
    "Emotional states heavily influence speech energy. For example, anger, happiness, and fear often result in louder, more energetic speech; sadness and boredom lower energy levels.\n",
    "\n",
    "**Why use consistent frame analysis parameters?**  \n",
    "By using `n_fft=1024` and `hop_length=512`, we align RMS measurement with other time-based features for coherence.\n",
    "\n",
    "**Why keep both mean and standard deviation?**  \n",
    "The mean energy indicates the general loudness level. The standard deviation reflects fluctuations in loudness across frames, which is key for capturing dynamic emotional expressions like sudden bursts of energy in anger or excitement.\n",
    "\n",
    "---\n",
    "\n",
    "#### Spectral Centroid\n",
    "\n",
    "**What is it?**  \n",
    "The spectral centroid measures the center of mass of the spectrum, indicating where most energy is concentrated in the frequency domain.\n",
    "\n",
    "**Why do we use spectral centroid?**  \n",
    "Brighter (higher centroid) signals are associated with energetic or excited emotions, while darker (lower centroid) signals indicate sadness or calmness.\n",
    "\n",
    "**Why use only the mean?**  \n",
    "Centroid values are generally stable within short utterances, and their average conveys enough discriminative power. Variance often adds noise rather than informative emotional cues, especially in clean recordings.\n",
    "\n",
    "---\n",
    "\n",
    "#### Spectral Bandwidth\n",
    "\n",
    "**What is it?**  \n",
    "Spectral bandwidth represents the spread of frequencies around the centroid.\n",
    "\n",
    "**Why do we use spectral bandwidth?**  \n",
    "A wider bandwidth indicates more tension and higher-frequency activity (typical in anger and excitement). Narrow bandwidth corresponds to calmer, more monotone speech.\n",
    "\n",
    "**Why use only the mean?**  \n",
    "Like the centroid, bandwidth is a relatively stable descriptor. The mean effectively captures this feature; standard deviation does not contribute meaningfully for short emotional utterances.\n",
    "\n",
    "---\n",
    "\n",
    "#### Spectral Rolloff\n",
    "\n",
    "**What is it?**  \n",
    "Spectral rolloff is the frequency below which a certain percentage (typically 85%) of the spectral energy lies.\n",
    "\n",
    "**Why do we use spectral rolloff?**  \n",
    "Higher rolloff values suggest more high-frequency energy, often found in tense, excited, or angry speech. Lower rolloff values are associated with soft, smooth speech, characteristic of sadness.\n",
    "\n",
    "**Why use only the mean?**  \n",
    "The rolloff tends to remain stable across frames. Variance often represents measurement noise rather than emotion-driven changes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Spectral Contrast\n",
    "\n",
    "**What is it?**  \n",
    "Spectral contrast measures the difference between peaks and valleys in the spectral envelope, computed across frequency sub-bands.\n",
    "\n",
    "**Why do we use spectral contrast?**  \n",
    "Emotionally expressive speech (anger, excitement) exhibits stronger contrast between prominent frequencies and valleys. Monotone speech (sadness) shows lower contrast.\n",
    "\n",
    "**Why keep both mean and standard deviation?**  \n",
    "The mean contrast indicates the overall dynamic range, while the standard deviation reveals frame-to-frame articulation dynamics. Emotional fluctuations are often well captured through std.\n",
    "\n",
    "---\n",
    "\n",
    "#### Chroma Features\n",
    "\n",
    "**What are they?**  \n",
    "Chroma features map the energy distribution into 12 pitch classes (like musical notes), offering a compact representation of pitch-based information.\n",
    "\n",
    "**Why do we use chroma features?**  \n",
    "Although speech is not melodic, emotional intonation shifts can cause systematic changes in pitch distribution. Emotions like excitement or happiness might emphasize certain pitch ranges, while sadness flattens pitch variation.\n",
    "\n",
    "**Why use only the mean?**  \n",
    "The mean chroma vector effectively captures the overall pitch class distribution. The standard deviation of chroma features typically adds little value for emotional analysis and can introduce redundancy or noise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== FEATURE EXCTRACTION FUNCTIONS WE USED ===================\n",
    "def compute_mfcc(y, sr, n_mfcc, n_fft, hop_length):\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    return mfcc\n",
    "\n",
    "    # mfcc_mean = mfcc.mean(axis=1)\n",
    "    # mfcc_std = mfcc.std(axis=1)\n",
    "    \n",
    "    # mfcc_delta = librosa.feature.delta(mfcc, order=1)\n",
    "    # delta_mean = mfcc_delta.mean(axis=1)\n",
    "    # delta_std = mfcc_delta.std(axis=1)\n",
    "    \n",
    "    # mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    # delta2_mean = mfcc_delta2.mean(axis=1)\n",
    "    # delta2_std = mfcc_delta2.std(axis=1)\n",
    "\n",
    "    # print(mfcc.shape)\n",
    "    \n",
    "    # return (mfcc_mean, mfcc_std, \n",
    "    #         delta_mean, delta_std,\n",
    "    #         delta2_mean, delta2_std)\n",
    "\n",
    "def compute_zcr(y, n_fft, hop_length):\n",
    "    zcr = librosa.feature.zero_crossing_rate(y, frame_length=n_fft, hop_length=hop_length)\n",
    "    return zcr\n",
    "\n",
    "def compute_rms(y, n_fft, hop_length):\n",
    "    rms = librosa.feature.rms(y=y, frame_length=n_fft, hop_length=hop_length)\n",
    "    return rms\n",
    "\n",
    "def compute_centroid(y, sr, n_fft, hop_length):\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    return centroid\n",
    "\n",
    "def compute_bandwidth(y, sr, n_fft, hop_length):\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    return bandwidth\n",
    "\n",
    "def compute_rolloff(y, sr, n_fft, hop_length):\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    return rolloff\n",
    "\n",
    "def compute_spectral_contrast(y, sr, n_fft, hop_length):\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    return spectral_contrast\n",
    "\n",
    "def compute_chroma(y, sr, n_fft, hop_length):\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    return chroma\n",
    "\n",
    "def compute_spectral_flux(y, sr, n_fft, hop_length):\n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    flux = np.sqrt(np.sum(np.diff(S, axis=1)**2, axis=0))\n",
    "    return flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_audio(file_path):\n",
    "    try:\n",
    "        y, _ = librosa.load(file_path, sr=SR)\n",
    "        \n",
    "        # MFCCs, MFCC Delta & Delta-Delta\n",
    "        mfcc = compute_mfcc(y, SR, N_MFCC, N_FFT, HOP_LENGTH)\n",
    "\n",
    "        # Zero Crossing Rate\n",
    "        zcr = compute_zcr(y, N_FFT, HOP_LENGTH)\n",
    "\n",
    "        # RMS Energy\n",
    "        rms = compute_rms(y, N_FFT, HOP_LENGTH)\n",
    "\n",
    "        # Spectral Features (only mean)\n",
    "        centroid = compute_centroid(y, SR, N_FFT, HOP_LENGTH)\n",
    "        bandwidth = compute_bandwidth(y, SR, N_FFT, HOP_LENGTH)\n",
    "        rolloff = compute_rolloff(y, SR, N_FFT, HOP_LENGTH)\n",
    "\n",
    "        # Spectral Contrast (mean + std)\n",
    "        contrast = compute_spectral_contrast(y, SR, N_FFT, HOP_LENGTH)\n",
    "\n",
    "        # Chroma Features (mean only)\n",
    "        chroma = compute_chroma(y, SR, N_FFT, HOP_LENGTH)\n",
    "        features = np.concatenate((\n",
    "            mfcc, zcr, rms, centroid, bandwidth, rolloff, contrast, chroma\n",
    "        ), axis=0)\n",
    "\n",
    "        aggregated_features = np.concatenate((\n",
    "            mfcc.mean(axis=1),  # Mean across the time axis for MFCC\n",
    "            [zcr.mean()],  # Mean of Zero Crossing Rate\n",
    "            [rms.mean()],  # Mean of RMS Energy\n",
    "            [centroid.mean()],  # Mean of Spectral Centroid\n",
    "            [bandwidth.mean()],  # Mean of Spectral Bandwidth\n",
    "            [rolloff.mean()],  # Mean of Spectral Roll-off\n",
    "            contrast.mean(axis=1),  # Mean across the time axis for Spectral Contrast\n",
    "            chroma.mean(axis=1)  # Mean across the time axis for Chroma\n",
    "        ))\n",
    "\n",
    "        return features, aggregated_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a clean, brief, and professional explanation of why each of these features was **not used** in your final extraction pipeline:  \n",
    "\n",
    "---\n",
    "\n",
    "### Why These Features Were Not Included:\n",
    "\n",
    "#### 1. **Spectral Flatness**  \n",
    "- **What it is:** Measures how noise-like or tone-like the signal is.  \n",
    "- **Why not used:**  \n",
    "  - It’s highly correlated with spectral contrast and ZCR, which we already use.  \n",
    "  - Adding it would introduce redundancy rather than new information, increasing feature dimensionality unnecessarily.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Tonnetz (Tonal Centroid Features)**  \n",
    "- **What it is:** Measures harmonic relations and pitch stability, mainly used in music.  \n",
    "- **Why not used:**  \n",
    "  - Emotional speech rarely follows harmonic structures like music.  \n",
    "  - This feature is not robust for short, non-musical utterances and may introduce noise into the feature set.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Spectral Polynomial Coefficients**  \n",
    "- **What it is:** Polynomial approximation of spectral shape.  \n",
    "- **Why not used:**  \n",
    "  - This feature is rarely used in speech emotion recognition research.  \n",
    "  - It is often unstable, version-dependent in `librosa`, and lacks literature support for SER.  \n",
    "  - Risk of overfitting without proven benefit.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Tempogram (tempo variation)**  \n",
    "- **What it is:** Describes tempo fluctuations based on onset strength.  \n",
    "- **Why not used:**  \n",
    "  - Emotional speech does not have strong rhythmic structures like music.  \n",
    "  - Utterances in CREMA-D are short and don't show tempo variations that would meaningfully correlate with emotion.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **F0 (Pitch) Estimation**  \n",
    "- **What it is:** Average pitch estimation from `pyin`.  \n",
    "- **Why not used:**  \n",
    "  - While pitch is emotionally relevant, `pyin` can be computationally heavy and unstable for noisy or short utterances.  \n",
    "  - RMS, spectral centroid, and MFCC deltas already capture pitch and energy dynamics indirectly but more robustly.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **HNR (Harmonic-to-Noise Ratio)**  \n",
    "- **What it is:** Ratio of periodic (harmonic) to noise components.  \n",
    "- **Why not used:**  \n",
    "  - Requires external libraries (Parselmouth), adding complexity and slowdowns.  \n",
    "  - In short speech samples, HNR estimates can be unreliable, and we already capture related information through spectral contrast and ZCR.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spectral Flatness (mean & std)\n",
    "# def compute_spectral_flatness(y, n_fft=1024, hop_length=512):\n",
    "#     flatness = librosa.feature.spectral_flatness(y=y, n_fft=n_fft, hop_length=hop_length)\n",
    "#     flatness_mean = flatness.mean()\n",
    "#     flatness_std = flatness.std()\n",
    "#     return flatness_mean, flatness_std\n",
    "\n",
    "# # Tonnetz (Tonal Centroid Features)\n",
    "# def compute_tonnetz(y, sr=16000):\n",
    "#     y_harmonic = librosa.effects.harmonic(y)\n",
    "#     tonnetz = librosa.feature.tonnetz(y=y_harmonic, sr=sr)\n",
    "#     tonnetz_mean = tonnetz.mean(axis=1)\n",
    "#     tonnetz_std = tonnetz.std(axis=1)\n",
    "#     return tonnetz_mean, tonnetz_std\n",
    "\n",
    "# # Spectral Polynomial Coefficients (if supported by your librosa version)\n",
    "# def compute_spectral_poly(y, sr=16000, n_fft=1024, hop_length=512, order=2):\n",
    "#     try:\n",
    "#         spec_poly = librosa.feature.spectral_polynomial(y=y, sr=sr, order=order, n_fft=n_fft, hop_length=hop_length)\n",
    "#         spec_poly_mean = spec_poly.mean(axis=1)\n",
    "#         spec_poly_std = spec_poly.std(axis=1)\n",
    "#         return spec_poly_mean, spec_poly_std\n",
    "#     except AttributeError:\n",
    "#         print(\"spectral_polynomial not available in this librosa version.\")\n",
    "#         return np.array([]), np.array([])\n",
    "\n",
    "# # Tempogram (tempo variation features)\n",
    "# def compute_tempogram(y, sr=16000, hop_length=512):\n",
    "#     onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "#     tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "#     tempogram_mean = tempogram.mean(axis=1)\n",
    "#     tempogram_std = tempogram.std(axis=1)\n",
    "#     return tempogram_mean, tempogram_std\n",
    "\n",
    "# # F0 (Pitch) Estimation using librosa.pyin\n",
    "# def compute_pitch(y, sr=16000):\n",
    "#     f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "#     f0_nonan = f0[~np.isnan(f0)]\n",
    "#     if len(f0_nonan) == 0:\n",
    "#         return 0.0, 0.0\n",
    "#     f0_mean = np.mean(f0_nonan)\n",
    "#     f0_std = np.std(f0_nonan)\n",
    "#     return f0_mean, f0_std\n",
    "\n",
    "# # Harmonic-to-Noise Ratio (HNR)\n",
    "# import parselmouth\n",
    "# def compute_hnr(y, sr=16000):\n",
    "#     try:\n",
    "#         sound = parselmouth.Sound(y, sampling_frequency=sr)\n",
    "#         pointProcess = sound.to_point_process_cc()\n",
    "#         hnr_values = sound.to_harmonicity_ac().values.T\n",
    "#         hnr_nonan = hnr_values[~np.isnan(hnr_values)]\n",
    "#         if len(hnr_nonan) == 0:\n",
    "#             return 0.0, 0.0\n",
    "#         hnr_mean = np.mean(hnr_nonan)\n",
    "#         hnr_std = np.std(hnr_nonan)\n",
    "#         return hnr_mean, hnr_std\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error computing HNR: {e}\")\n",
    "#         return 0.0, 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeLine so that each audio gets its features extracted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "aggregated_features = []\n",
    "filenames = []\n",
    "emotions = []\n",
    "\n",
    "for file in os.listdir(preprocessed_dataset_path):\n",
    "    if file.lower().endswith(\".wav\"):\n",
    "        path = os.path.join(preprocessed_dataset_path, file)\n",
    "        result, aggregated_result = extract_features_from_audio(path)\n",
    "        if result is not None and aggregated_result is not None:\n",
    "            parts = file.split('_')\n",
    "            emotion = parts[2]\n",
    "            print(f\"features are extracted from : {file}\")\n",
    "            feature_vector = [list(feature) for feature in result]\n",
    "            features.append(feature_vector)\n",
    "            aggregated_features.append(aggregated_result)\n",
    "            filenames.append(file)\n",
    "            emotions.append(emotion)\n",
    "\n",
    "# print(\"Number of audio files:\", len(features))\n",
    "# print(\"Number of features per file:\", len(features[0])) \n",
    "# print(\"Number of frames per feature:\", len(features[0][0])) \n",
    "\n",
    "# Feature column names\n",
    "mfcc_cols = [f\"mfcc{i+1}\" for i in range(13)]\n",
    "\n",
    "spec_cols = [\n",
    "    \"zcr\",\n",
    "    \"rms\",\n",
    "    \"centroid\",\n",
    "    \"bandwidth\",\n",
    "    \"rolloff\"\n",
    "]\n",
    "\n",
    "contrast_cols = [f\"spectral_contrast_band{i+1}\" for i in range(7)] \n",
    "\n",
    "chroma_cols = [f\"chroma{i+1}\" for i in range(12)]\n",
    "\n",
    "columns = [\"file_name\"] + mfcc_cols + spec_cols + contrast_cols + chroma_cols + [\"emotion\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_emotions = encoder.fit_transform(emotions)\n",
    "\n",
    "all_feature_names = mfcc_cols + spec_cols + contrast_cols + chroma_cols\n",
    "\n",
    "df = pd.DataFrame(features, columns=all_feature_names)\n",
    "df.insert(0, \"file_name\", filenames)\n",
    "df['emotion'] = encoded_emotions\n",
    "df.to_csv(csv_features_path, index=False)\n",
    "\n",
    "df_aggregated = pd.DataFrame(aggregated_features, columns=all_feature_names)\n",
    "df_aggregated.insert(0, \"file_name\", filenames)\n",
    "df_aggregated['emotion'] = encoded_emotions\n",
    "df_aggregated.to_csv(csv_aggregated_features_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_aggregated_features_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df.iloc[:, 1:-1]  # Features: all columns except first (filename) and last (target)\n",
    "y = df.iloc[:, -1]    # Target: last column\n",
    "\n",
    "# Compute Mutual Information\n",
    "mutual_info = mutual_info_classif(X, y)\n",
    "\n",
    "# Compute Correlation (Pearson)\n",
    "correlations = X.corrwith(y)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "mi_corr_df = pd.DataFrame({ \n",
    "    \"Feature\": X.columns,\n",
    "    \"Mutual Information\": mutual_info,\n",
    "    \"Correlation\": correlations\n",
    "})\n",
    "\n",
    "mi_corr_df.to_csv(\"Analytics/mi_corr_aggregated_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Frame     Frame_Name  Correlation  Mutual Information\n",
      "0   mfcc1      0  mfcc1_frame_0     0.039903            0.013878\n",
      "1   mfcc1      1  mfcc1_frame_1     0.035236            0.008764\n",
      "2   mfcc1      2  mfcc1_frame_2     0.020226            0.005928\n",
      "3   mfcc1      3  mfcc1_frame_3    -0.004387            0.006149\n",
      "4   mfcc1      4  mfcc1_frame_4    -0.030510            0.015754\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_features_path)\n",
    "\n",
    "feature_columns = df.columns[1:-1]\n",
    "results = []\n",
    "\n",
    "# Loop through each feature\n",
    "for feature_name in feature_columns:\n",
    "    # Parse the list into 2D array\n",
    "    feature_series = df[feature_name].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    X = np.stack(feature_series.values)\n",
    "\n",
    "    # Create frame-wise DataFrame\n",
    "    frame_df = pd.DataFrame(X, columns=[f\"{feature_name}_frame_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    # Pearson correlation\n",
    "    corr = frame_df.corrwith(y)\n",
    "\n",
    "    # Mutual information\n",
    "    mi = mutual_info_classif(frame_df, y, discrete_features=False)\n",
    "\n",
    "    # Combine into result list\n",
    "    for i in range(X.shape[1]):\n",
    "        results.append({\n",
    "            \"Feature\": feature_name,\n",
    "            \"Frame\": i,\n",
    "            \"Frame_Name\": f\"{feature_name}_frame_{i}\",\n",
    "            \"Correlation\": corr.iloc[i],\n",
    "            \"Mutual Information\": mi[i]\n",
    "        })\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "mi_corr_df = pd.DataFrame(results)\n",
    "\n",
    "mi_corr_df.to_csv(\"Analytics/framewise_mi_corr_all_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mfcc_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m framewise_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalytics/framewise_mi_corr_all_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m aggregated_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalytics/mi_corr_aggregated_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m all_feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mmfcc_cols\u001b[49m \u001b[38;5;241m+\u001b[39m spec_cols \u001b[38;5;241m+\u001b[39m contrast_cols \u001b[38;5;241m+\u001b[39m chroma_cols\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# plot_dir = \"plot_feature_mi_corr\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# os.makedirs(plot_dir, exist_ok=True)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# for feature in all_feature_names:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     plot_feature_mi_corr(feature, framewise_df, aggregated_df) \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mfcc_cols' is not defined"
     ]
    }
   ],
   "source": [
    "framewise_df = pd.read_csv(\"Analytics/framewise_mi_corr_all_features.csv\")\n",
    "aggregated_df = pd.read_csv(\"Analytics/mi_corr_aggregated_features.csv\")\n",
    "\n",
    "plot_dir = \"plot_feature_mi_corr\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "def plot_feature_mi_corr(feature_name, framewise_df, aggregated_df):\n",
    "    # Filter data\n",
    "    df_feat = framewise_df[framewise_df[\"Feature\"] == feature_name].reset_index(drop=True)\n",
    "\n",
    "    agg_row = aggregated_df[aggregated_df[\"Feature\"] == feature_name].reset_index(drop=True)\n",
    "    agg_mi = agg_row[\"Mutual Information\"].values[0]\n",
    "    agg_corr = abs(agg_row[\"Correlation\"].values[0])\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "    # MI plot\n",
    "    ax1.bar(df_feat[\"Frame\"], df_feat[\"Mutual Information\"], color=\"skyblue\", alpha=0.8, label=\"MI per Frame\")\n",
    "    ax1.axhline(y=agg_mi, color=\"blue\", linestyle=\"--\", linewidth=2, label=\"Aggregated MI\")\n",
    "    ax1.set_title(f\"Mutual Information — {feature_name}\")\n",
    "    ax1.set_xlabel(\"Frame\")\n",
    "    ax1.set_ylabel(\"Mutual Information\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Correlation plot\n",
    "    ax2.bar(df_feat[\"Frame\"], np.abs(df_feat[\"Correlation\"]), color=\"salmon\", alpha=0.8, label=\"|Correlation| per Frame\")\n",
    "    ax2.axhline(y=agg_corr, color=\"red\", linestyle=\"--\", linewidth=2, label=\"|Aggregated Correlation|\")\n",
    "    ax2.set_title(f\"Absolute Correlation — {feature_name}\")\n",
    "    ax2.set_xlabel(\"Frame\")\n",
    "    ax2.set_ylabel(\"|Correlation|\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_dir}/{feature_name}_mi_corr_plot.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "for feature in all_feature_names:\n",
    "    plot_feature_mi_corr(feature, framewise_df, aggregated_df) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your feature CSV again\n",
    "df = pd.read_csv(csv_features_path)  # replace with actual path if needed\n",
    "pca_analysis_dir = \"pca_analysis\"\n",
    "os.makedirs(pca_analysis_dir, exist_ok=True)\n",
    "\n",
    "# Define the plotting function\n",
    "def plot_pca_for_feature(df_column, feature_name):\n",
    "    feature_series = df_column.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    X = np.stack(feature_series.values)\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(explained, marker='o')\n",
    "    plt.title(f\"PCA - Cumulative Variance for {feature_name}\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Variance Explained\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{pca_analysis_dir}/{feature_name}_correlation_heatmap.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Loop and plot PCA for each feature column\n",
    "for feature_name in all_feature_names:\n",
    "    plot_pca_for_feature(df[feature_name], feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_dir = \"correlation_matrix_between_time_frames\"\n",
    "os.makedirs(heatmap_dir, exist_ok=True)\n",
    "\n",
    "def plot_correlation_matrix(feature_name):\n",
    "    feature_series = df[feature_name].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    X = np.stack(feature_series.values)\n",
    "\n",
    "    X_first_10 = X[:, :10]\n",
    "    corr_matrix = np.corrcoef(X_first_10.T)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0, square=True, annot=True, fmt=\".2f\")\n",
    "    plt.title(f\"Correlation Matrix of {feature_name}\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Time Step\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"{heatmap_dir}/{feature_name}_correlation_heatmap.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Run the analysis for each feature\n",
    "for feature_name in all_feature_names:\n",
    "    plot_correlation_matrix(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features saved\n"
     ]
    }
   ],
   "source": [
    "framewise_MI_corr_df = pd.read_csv(\"Analytics/framewise_mi_corr_all_features.csv\")\n",
    "aggregated_MI_corr_df = pd.read_csv(\"Analytics/mi_corr_aggregated_features.csv\")\n",
    "df_aggregated_features = pd.read_csv(csv_aggregated_features_path)\n",
    "df_framewise_features = pd.read_csv(csv_features_path)\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Loop through all features\n",
    "for feature in all_feature_names:\n",
    "    # Get aggregated MI\n",
    "    agg_mi = aggregated_MI_corr_df.loc[aggregated_MI_corr_df[\"Feature\"] == feature, \"Mutual Information\"].values[0]\n",
    "    \n",
    "    # Get max framewise MI\n",
    "    max_frame_mi = framewise_MI_corr_df[framewise_MI_corr_df[\"Feature\"] == feature][\"Mutual Information\"].max()\n",
    "\n",
    "    # Choose version with higher MI\n",
    "    if max_frame_mi > agg_mi:\n",
    "        final_df[feature] = df_framewise_features[feature]\n",
    "    else:\n",
    "        final_df[feature] = df_aggregated_features[feature]\n",
    "\n",
    "final_df.insert(0, \"file_name\", df_framewise_features[\"file_name\"])\n",
    "final_df[\"emotion\"] = df_framewise_features[\"emotion\"]\n",
    "\n",
    "\n",
    "final_df.to_csv(csv_final_features_path, index=False)\n",
    "\n",
    "print(\"Final features saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
